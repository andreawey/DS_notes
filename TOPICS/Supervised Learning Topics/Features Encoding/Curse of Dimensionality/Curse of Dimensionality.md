[[Features Encoding]] becomes crucial when we have too many features

More features make the model more expressive
	but not all of the features are relevant
Adding more features makes a model more complex, and so increases the chance of overfitting.
The higher the dimensionality, the higher the chances of spurious features

With high dimensional datasets it is a good idea to reduce the number of features to only the most useful ones.

To counter the curse of dimensionality it is required to do [[Feature selection]] 