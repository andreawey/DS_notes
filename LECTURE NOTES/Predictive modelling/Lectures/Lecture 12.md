# Bayesian Structural Time Series (BSTS)

## State Space Models - Core Principles

> [!important] Two Fundamental Principles
> 
> 1. **Hidden/Latent Process** $X_t$ is the state process (Markov property)
>     - Future states ${x_s; s > t}$ depend only on present state $x_t$
>     - Independent of past sequence ${x_s; s < t}$
> 2. **Conditional Independence**
>     - Observations $Y_t$ are independent given states $X_t$
>     - Dependence among observations is generated by states

## State Space Model Equations

> [!note] Core Model Structure State space models are built on two equations:

**State Equation:** $$X_t = G_t X_{t-1} + W_t$$ where $W_t \sim N(0, w_t)$

**Observation Equation:** $$Y_t = F_t X_t + V_t$$ where $V_t \sim N(0, v_t)$

> [!tip] Matrix Properties
> 
> - All matrices $G_t$, $F_t$, $w_t$, $v_t$ can be time-varying
> - Often time-constant, with $F_t$ most likely to adapt over time

### AR(1) Example with Measurement Noise

**State equation:** $$X_t = \alpha_1 X_{t-1} + W_t$$

**Observation equation:** $$Y_t = X_t + V_t$$

> [!important] Noise Distinction
> 
> - **Process noise** $W_t$: affects all future values $X_{t+k}$ and $Y_{t+k}$
> - **Measurement noise** $V_t$: only influences current observation $Y_t$

## BSTS Model Components

> [!success] Why BSTS is Powerful
> 
> - **Flexibility**: Components (trend, seasonality, regressors) can be included/excluded
> - **Explicit Modeling**: Underlying components can be modeled explicitly and visually inspected
> - **Uncertainty Quantification**: Posterior uncertainty quantified for each component
> - **Prior Integration**: Prior beliefs incorporated directly into the model

### Complete BSTS Model Specification

**Observation Equation:** $$Y_t = F_t \mu_t + \tau_t + \beta_t x_t + \varepsilon_t$$

where $\varepsilon_t \sim N(0, \Sigma_\varepsilon)$

> [!info] Component Definitions
> 
> - $Y_t \in \mathbb{R}^n$: observed value at time $t$
> - $\mu_t \in \mathbb{R}^p$: local level (trend) component
> - $F_t \in \mathbb{R}^{n \times p}$: output matrix
> - $\tau_t \in \mathbb{R}^n$: seasonal component
> - $\beta_t x_t \in \mathbb{R}^n$: regression term with time-varying coefficients
> - $\varepsilon_t$: observation noise (white noise)

### State Equations

**Local Linear Trend:** $$\mu_{t+1} = G_t \mu_t + \eta_t, \quad \eta_t \sim N(0, \Sigma_\eta)$$

**Extended Trend with Slope:** $$\begin{align} \mu_{t+1} &= \mu_t + \delta_t + \eta_t \ \delta_{t+1} &= \delta_t + \zeta_t \end{align}$$

where:

- $\delta_t \in \mathbb{R}$: local slope (expected change in $\mu_t$)
- $\zeta_t \sim N(0, \Sigma_\zeta)$

> [!note] Trend Behavior Due to $\delta_t$: Trend behaves like integrated random walk with drift, capturing recent momentum

**Seasonality:** $$\tau_{t+1} = -\sum_{j=1}^{S-1} \tau_{t-j} + \omega_t$$

where:

- $S$: season length (e.g., $S = 4$ for quarterly data)
- $\omega_t$: noise term allowing seasonal pattern evolution
- Constraint: $\sum_{j=0}^{S} \tau_{t-j} = 0$

### Local Level Model (Simplest Case)

$$\begin{align} Y_t &= \mu_t + \varepsilon_t \quad \text{(observation equation)} \ \mu_{t+1} &= \mu_t + \eta_t \quad \text{(state equation)} \end{align}$$

> [!abstract] Interpretation This represents a random walk observed in noise

## Bayesian Variable Selection

> [!important] Spike-and-Slab Priors Used for automatic variable selection from large, possibly correlated predictor sets

**Key Benefits:**

- Automatic selection of relevant variables
- Parsimonious models balancing fit and interpretability
- Incorporation of prior beliefs about coefficient inclusion
- Proper uncertainty accounting in forecasts

> [!info] Spike-and-Slab Components
> 
> - **Spike**: Governs probability of variable inclusion (positive probability of being zero)
> - **Slab**: Shrinks non-zero coefficients toward prior expectations (similar to Ridge regression)

**Result:** Posterior draws often have many coefficients exactly zero (unlike Lasso priors)

## Causal Inference with BSTS

### Potential Outcomes Framework

**Causal Effect for Unit $i$:** $$\tau_i = Y_i(1) - Y_i(0)$$

where:

- $Y_i(1)$: potential outcome under treatment
- $Y_i(0)$: potential outcome under control

> [!warning] Fundamental Problem of Causal Inference
> 
> - **Factual outcome**: observed outcome under given treatment
> - **Counterfactual outcome**: unobservable outcome under different treatment
> - We can only observe one potential outcome per unit

### CausalImpact Method

> [!success] CausalImpact Approach Uses BSTS to estimate causal effects when randomized experiments aren't available

**Process:**

1. Use response time series and control time series
2. Construct Bayesian structural time-series model
3. Predict counterfactual: how response would have evolved without intervention
4. Compare actual vs. counterfactual to estimate causal effect

**Key Applications:**

- Advertising campaign effectiveness
- Policy intervention analysis
- Treatment effect estimation

> [!example] Example Use Case "How many additional daily clicks were generated by an advertising campaign?"

## Mathematical Notation Summary

|Symbol|Description|
|---|---|
|$X_t$|State/latent process at time $t$|
|$Y_t$|Observed value at time $t$|
|$G_t$|State transition matrix|
|$F_t$|Observation matrix|
|$W_t$|Process noise|
|$V_t$|Measurement noise|
|$\mu_t$|Local level/trend component|
|$\tau_t$|Seasonal component|
|$\beta_t$|Time-varying regression coefficients|
|$\varepsilon_t$|Observation error|
|$\eta_t$|State innovation|
|$\omega_t$|Seasonal innovation|
