### Training data requires careful selection criteria
1. **Diversity & balance**
   - Varied contexts, languages, dialects and styles for the desired application
   - Avoiding over-representation of certain classes or topics to prevent bias
2. **Quality & Quantity**
   - Accurate, consistently annotated and free of ambiguities, errors or biases
   - Sufficient data volume to capture underlying patterns and structures
3. **Relevance & Recency**
   - Aligned with the target task or domain of the specific application
   - Including up-to-date data and current knowledge to avoid obsolescence

>[!NOTE] Examples of benchmarks
> >| GLUE | General Language Understanding | Collection of 9 language understanding tasks, single sentence, similarity, paraphrasing and NLI|
> >| ----- | ----- | ----- |
> >| SNLI | Stanford Natural Language Inference | Collection of sentence pairs labeled for entailment, contradiction and neutral relationships for NLI models|
> >| SQuAD | Stanford Question Answering Dataset| Reading comprehension dataset of questions posed on Wikipedia articles, answer is segment of text|
> >|SCROLLS | Standardized Comparison Over Long Language Sentences|NLP benchmark consisting of a suit of tasks that require reasoning over long texts, including summarization, QA and NLI|
> >|CoNLL-2003| CoNLL-2003| Named entity recognition (NER) dataset released with paper consisting of eight files covering English and German|

>[!FAQ] Sources
>Hugging Face has datasets for different task and application 
>



