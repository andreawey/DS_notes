
Graph neural network s : intro and motivation

graphs may come from nature, in many applications graphs come naturally.
For example for chemical bonds.

you could process it as a vector but you lose the graphical dependency.
In social networks we are virtual sensors producing data, creating relationships.

dependency can be build using pearson correlation as indicator between notes.
Granger causality one node's data can be used to reproduce another node's signal.

graph stream shows the evolution of the graph over time, graph stream is what we need to process

Examples:
- molecules
- physical system that can be decomposed in nodes
- rigid body system where bodies interact
- sentence and parse trees
- image and fully connected scene graph

you introduce dependencies between objects in an environment.


how is a graph represented? 

binary adjacency matrix
node features
edge features


Slide 38 graph shift operator 
has value 0 for all nodes that do not have a connection with the node and 0 for itself

X graph signal matrix is a matrix that contains:
- first line features at the node level
- second line fatures associated to the second node 
- ...

represents all the information of all the nodes on the graph

X is a set of vectors
Ã is an operator

applying Ã to X gives X' 
see slide 39


$x'_i$  the new value of node i is the contribution coming from the neighbors
for all *j* (info of the neightbors) will represent the contribution of the neighbors
$ã_{ji} * x_j$  changing the state of x_i looking at the neighbors

$\theta$ is a matrix that multiplied to X' gives a matrix H that is a convolution 
$H = ÃX\theta$ 
H is a Square matrix X rectangular matrix X rectangular matrix

for ith node 
$h_i = \sum ã_{ji} * x_j\theta$




